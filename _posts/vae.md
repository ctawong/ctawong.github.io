---
title: Variational autoencoder
layout: post
---
## Theory
### Model
The digit images ($x$) are generated by an unknown process $p_{\theta^*}(x|z)$, where $z$ is an unobservable latent variable. Here we plug in a specific $z$ into this function, and it specifies the intensity *distribution* of the image pixels of $x$. 

We don't know the true parameter $\theta^*$ but we know the general form of the parametric function $p_{\theta}(x|z)$. Note that $\theta$ denotes variable of parameters, while $\theta^*$ denotes the truth value. Latent variable $z$ follows distribution $p_{\theta^*}(z)$. 

The *likelihood function* $p_{\theta}(x)$ measures how well $\theta$ describes the observed digit $x$.  By definition $p_{\theta^*}(x) = 1$ because $\theta^*$ generates the observation. However, $p_{\theta}(x)$ is intractable, meaning that we cannot evaluate or differentiate $p_{\theta}(x)$ for every $\theta$ and $x$.  Same for $p_{\theta}(x|z)$ and $p_{\theta}(z|x)$.  Since $p_{\theta}(z|x)$ is hard to evaluate,  we introduce a new function $q_{\phi}(z|x)$ to approximate it.  

|variable | Description   | 
|---------|--------------|
| $x$     | image of digit        | 
| $z$     | latent variable that generates the image        | 
| $\theta$     | Parameter of the true model  $p_{\theta}$       | 
|$\theta^*$  |  True parameter. $p_{\theta^*}(x)=1$ is maximum|
|$p_{\theta}(x)$  |  Likelihood function - How likely $\theta$ describes $x$|
|$p_{\theta}(z)$  |  Prior distribution of $z$|
|$p_{\theta}(x\vert z)$  |  Likelihood or generative function of $x$ given $z$|
|$p_{\theta}(z\vert x)$  |  Likelihood or generative function of $z$ given $x$|
|$q_{\phi}(z\vert x)$  |  Approximate function to $p_{\theta}(z\vert x)$ |

### Objective function
In variational auto encoder, the objective function is the likelihood function of the true model $p_{\theta}(x)$.  Ideally we want to find the optimal $\theta$ that best matches the observed images $x$.  $p_{\theta}(x)$ can be expressed in terms of Kullback-Leiber divergence $D_{KL}$.  $D_{KL}(q_{\phi}(z|x) ||  p_{\theta(z|x) })$ measures how well $q_{\phi}(z|x)$ approximates  $p_{\theta(z|x)}$ :

$$
D_{KL}(q_{\phi}(z|x) ||  p_{\theta(z|x) })  = \mathbb{E}_{q_{\phi}(z|x)}[\ln q_{\phi}(z|x)-\ln p(x,z) + \ln p_{\theta}(x)]
$$
or
$$
\ln p_{\theta}(x) = D_{KL}(q_{\phi}(z|x) ||  p_{\theta(z|x) })  + \mathcal{L}.
$$ where 
$$
\mathcal{L} = \mathbb{E}_{q_{\phi}(z|x)}[-\ln q_{\phi}(z|x)+\ln p(x,z) ]
$$
$\ln p_{\theta}(x)$ can be taken out of the expectation because it does not depend on $z$. Since D_{KL} is non-negative, $\mathcal{L}$ serves as a *lower bound* to $\ln p_{\theta}(x)$ .
$$
\ln p_{\theta}(x)  \ge \mathcal{L}.
$$

**model**:  from z to digital
**variation**: from digit x to latent variable z.

## Model class

`model` takes latent variable $z$  and generates digit $x$
```python
class Model(nn.Module):
  """Bernoulli model parameterized by a generative network with Gaussian latents for MNIST."""
  def __init__(self, latent_size, data_size):
    super().__init__()
    self.register_buffer('p_z_loc', torch.zeros(latent_size))
    self.register_buffer('p_z_scale', torch.ones(latent_size))
    self.log_p_z = NormalLogProb()
    self.log_p_x = BernoulliLogProb()
    self.generative_network = NeuralNetwork(input_size=latent_size,
                                            output_size=data_size, 
                                            hidden_size=latent_size * 2)
```
$x$: digit image (size: 28x28 = 784)
$z$: latent variable (size: 128)
`log_p_z`:  $\log p(z)$ is log normal distribution with zero mean and unit variance.\
`log_p_x`: $\log p(x)$ is log Bernoulli distribution with logits, i.e. $y\log \sigma(x) + (1-y)\log[1-\sigma(x)]$, $\sigma$ is the Sigmoid function. $y$ is probability of a bright pixel.
`generative_network` takes  latent variable $z$ and generates a digit image $x$. Note that need to pass x through a Sigmoid function (i.e. $\sigma(x)$ ) to form a digit with intensity range 0 to 1. 

   ### Feed forward of Model
```python
  def forward(self, z, x):
    """Return log probability of model.
       Generates logits from z, then compare with truth x
    """
    log_p_z = self.log_p_z(self.p_z_loc, self.p_z_scale, z).sum(-1, keepdim=True)
    logits = self.generative_network(z)
    # unsqueeze sample dimension
    logits, x = torch.broadcast_tensors(logits, x.unsqueeze(1))
    log_p_x = self.log_p_x(logits, x).sum(-1, keepdim=True) # note that this is function of both generated logits and truth x
    return log_p_z + log_p_x
```
$\log p(z)$ is the prior probability of observing $z$.  It already sums over the dimension of $z$.  We can do that because $p(z) = p(z_1)p(z_2)...p(z_n)$ for normal distribution.  So we have one value for each z.  
`logits`  $x_z$ is generated by the model using $z$. It is the generated digit image.

$\log p( \sigma(x_z), x )$ is function that compare $\sigma(x_z)$ and $x$.

* $p(0,0) = 0$
* $p(1,1) = 0$
* $p(-1 ,1) = -27.6$
* $p(1 ,-1) = -27.6$

for each digit.  So $\log p( \sigma(x_z), x )$ is maximized when the generated digit matches the truth. Note `log_p_x` is  $\log p( \sigma(x_z), x )$ with negative sign included.

The `forward` function returns $p_{xz}(x,z)$, the joint prior probability of $x$ and $z$.  Using $\log p_{xz}(x,z) = \log p_x(x) p_z(z) = \log p_x(x) + \log p_z(z)$, the `forward` function returns $\log p_z(z) + \log p( \sigma(x_z),x )$.  It has dimension of the batch size (128).
## Variational class

`VariationalMeanField` takes an image $x$ and returns the latent variable $z$

```python
class VariationalMeanField(nn.Module):
  """Approximate posterior parameterized by an inference network."""
  def __init__(self, latent_size, data_size):
    super().__init__()
    self.inference_network = NeuralNetwork(input_size=data_size, 
                                           output_size=latent_size * 2, 
                                           hidden_size=latent_size*2)
    self.log_q_z = NormalLogProb()
    self.softplus = nn.Softplus()
```

[Softplus](https://pytorch.org/docs/stable/nn.html#softplus) is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.


```python
  def forward(self, x, n_samples=1):
    """Return sample of latent variable and log prob."""
    loc, scale_arg = torch.chunk(self.inference_network(x).unsqueeze(1), chunks=2, dim=-1) # Infer parameters of z from x (loc and scale)
    scale = self.softplus(scale_arg) # make sure scale is positive
    eps = torch.randn((loc.shape[0], n_samples, loc.shape[-1]), device=loc.device)
    z = loc + scale * eps  # reparameterization
    log_q_z = self.log_q_z(loc, scale, z).sum(-1, keepdim=True)
    return z, log_q_z
```
`inference_network` returns a vector two times of size of latent variable.  It takes a digit image $x$ as input and output the *parameters* of probability distribution of latent variable $z$. i.e. mean (`loc`) and variance (`scale`)  which have dimension of latent variable $z$.

The reparameterization trick is to redraw the latent variable $z$ from normal distribution with parameters (i.e. `loc` and `scale`) inferred from the original $z$.  This has effect of increasing the variance of $z$.

$\log q(z)$ (`log_q_z`) is the log probability of the new $z$, one value per image.

## Steps

```python
  for step, batch in enumerate(cycle(train_data)):
    x = batch[0].to(device)
    model.zero_grad()
    variational.zero_grad()
    z, log_q_z = variational(x, n_samples=1)
    log_p_x_and_z = model(z, x)
    # average over sample dimension
    elbo = (log_p_x_and_z - log_q_z).mean(1)
    # sum over batch dimension
    loss = -elbo.sum(0)
    loss.backward()
    optimizer.step()
```

 1. Infer latent variable $z$ from $x$. Return the reparameterized $z$ and its log probability $\log q(z)$
 2.  Generative model (`model`)  uses reparameterized $z$  to get  $\log p(z) - \log p( \sigma(x_z), x )$ (`log_p_x_and_z`)
 3. ELBO = $\log p(z) +\log p( \sigma(x_z),x ) - \log q(z)$
